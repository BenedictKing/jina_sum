# encoding:utf-8
import html
import json
import random
import re
import time
import xml.etree.ElementTree as ET
from urllib.parse import parse_qs, urlparse

import nest_asyncio
import newspaper
import requests
from bs4 import BeautifulSoup
from newspaper import Article
from requests_html import HTMLSession

import plugins
from bridge.context import ContextType
from bridge.reply import Reply, ReplyType
from common.log import logger
from common.utils import remove_markdown_symbol
from plugins import Event, EventAction, EventContext, Plugin

# åº”ç”¨nest_asyncioä»¥è§£å†³äº‹ä»¶å¾ªç¯é—®é¢˜
try:
    nest_asyncio.apply()
except Exception as e:
    logger.warning(f"[JinaSum] æ— æ³•åº”ç”¨nest_asyncio: {str(e)}")


@plugins.register(
    name="JinaSum",
    desire_priority=20,
    hidden=False,
    desc="Sum url link content with newspaper3k and llm",
    version="2.4.1",
    author="BenedictKing",
)
class JinaSum(Plugin):
    """ç½‘é¡µå†…å®¹æ€»ç»“æ’ä»¶

    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨æ€»ç»“åˆ†äº«çš„ç½‘é¡µå†…å®¹
    2. æ”¯æŒæ‰‹åŠ¨è§¦å‘æ€»ç»“
    3. æ”¯æŒç¾¤èŠå’Œå•èŠä¸åŒå¤„ç†æ–¹å¼
    4. æ”¯æŒé»‘åå•ç¾¤ç»„é…ç½®
    """

    # é»˜è®¤é…ç½®
    DEFAULT_CONFIG = {
        "max_words": 8000,
        "prompt": "æˆ‘éœ€è¦å¯¹ä¸‹é¢å¼•å·å†…æ–‡æ¡£è¿›è¡Œæ€»ç»“ï¼Œæ€»ç»“è¾“å‡ºåŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š\nğŸ“– ä¸€å¥è¯æ€»ç»“\nğŸ”‘ å…³é”®è¦ç‚¹,ç”¨æ•°å­—åºå·åˆ—å‡º3-5ä¸ªæ–‡ç« çš„æ ¸å¿ƒå†…å®¹\nğŸ· æ ‡ç­¾: #xx #xx\nè¯·ä½¿ç”¨emojiè®©ä½ çš„è¡¨è¾¾æ›´ç”ŸåŠ¨\n\n",
        "white_url_list": [],
        "black_url_list": [
            "https://support.weixin.qq.com",
            "https://channels-aladin.wxqcloud.qq.com",
            "https://www.wechat.com",
            "https://channels.weixin.qq.com",
            "https://docs.qq.com",
            "https://work.weixin.qq.com",
            "https://map.baidu.com",
            "https://map.qq.com",
            "https://y.qq.com",
            "https://music.163.com",
        ],
        "black_group_list": [],
        "auto_sum": True,
        "cache_timeout": 900,  # ç¼“å­˜è¶…æ—¶æ—¶é—´ï¼ˆ15åˆ†é’Ÿï¼‰
        "openai_api_base": "https://api.openai.com/v1",
        "openai_api_key": "",
        "openai_model": "gpt-4o-2024-08-06",
        "qa_prompt": "æ ¹æ®ä»¥ä¸‹æ–‡ç« å†…å®¹å›ç­”é—®é¢˜ï¼š\n'''{content}'''\n\né—®é¢˜ï¼š{question}\nè¦æ±‚ï¼šç­”æ¡ˆéœ€å‡†ç¡®ç®€æ´ï¼Œå¼•ç”¨åŸæ–‡å†…å®¹éœ€ç”¨å¼•å·æ ‡æ³¨",
        "qa_trigger": "é—®",
    }

    def __init__(self):
        """åˆå§‹åŒ–æ’ä»¶é…ç½®"""
        try:
            super().__init__()

            # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®ï¼ˆç”¨æˆ·é…ç½®è¦†ç›–é»˜è®¤ï¼‰
            user_config = super().load_config() or {}
            self.config = {**self.DEFAULT_CONFIG, **user_config}  # ç¡®ä¿ç”¨æˆ·é…ç½®ä¼˜å…ˆ

            # ç±»å‹éªŒè¯å’Œè½¬æ¢
            self.max_words = int(self.config["max_words"])
            self.prompt = str(self.config["prompt"])
            self.cache_timeout = int(self.config["cache_timeout"])
            self.auto_sum = self.config["auto_sum"]

            # APIé…ç½®å¤„ç†
            self.openai_api_base = str(self.config["openai_api_base"]).rstrip("/")
            self.openai_api_key = str(self.config["openai_api_key"])
            self.openai_model = str(self.config["openai_model"])  # ä¿æŒå˜é‡åä¸€è‡´æ€§

            # åˆ—è¡¨ç±»å‹é…ç½®å¤„ç†
            self.white_url_list = list(map(str, self.config["white_url_list"]))
            self.black_url_list = list(map(str, self.config["black_url_list"]))
            self.black_group_list = list(map(str, self.config["black_group_list"]))

            # é—®ç­”ç›¸å…³é…ç½®
            self.qa_prompt = str(self.config["qa_prompt"])
            self.qa_trigger = str(self.config["qa_trigger"])  # ä¿®å¤å˜é‡åæ‹¼å†™é”™è¯¯

            # æ¶ˆæ¯ç¼“å­˜
            self.pending_messages = {}  # ç”¨äºå­˜å‚¨å¾…å¤„ç†çš„æ¶ˆæ¯ï¼Œæ ¼å¼: {chat_id: {"content": content, "timestamp": time.time()}}
            self.content_cache = {}  # ç”¨äºå­˜å‚¨å·²å¤„ç†çš„å†…å®¹ç¼“å­˜ï¼Œæ ¼å¼: {url: {"content": content, "timestamp": time.time()}}

            logger.info("[JinaSum] åˆå§‹åŒ–å®Œæˆ")
            self.handlers[Event.ON_HANDLE_CONTEXT] = self.on_handle_context
        except Exception as e:
            logger.error(f"[JinaSum] åˆå§‹åŒ–å¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            raise Exception("[JinaSum] åˆå§‹åŒ–å¤±è´¥")

    def on_handle_context(self, e_context: EventContext):
        """å¤„ç†æ¶ˆæ¯"""
        context = e_context["context"]
        logger.info(f"[JinaSum] æ”¶åˆ°æ¶ˆæ¯, ç±»å‹={context.type}, å†…å®¹é•¿åº¦={len(context.content)}")

        # é¦–å…ˆåœ¨æ—¥å¿—ä¸­è®°å½•å®Œæ•´çš„æ¶ˆæ¯å†…å®¹ï¼Œä¾¿äºè°ƒè¯•
        orig_content = context.content
        if len(orig_content) > 500:
            logger.info(f"[JinaSum] æ¶ˆæ¯å†…å®¹(æˆªæ–­): {orig_content[:500]}...")
        else:
            logger.info(f"[JinaSum] æ¶ˆæ¯å†…å®¹: {orig_content}")

        if context.type not in [ContextType.TEXT, ContextType.SHARING]:
            logger.info(f"[JinaSum] æ¶ˆæ¯ç±»å‹ä¸ç¬¦åˆå¤„ç†æ¡ä»¶ï¼Œè·³è¿‡: {context.type}")
            return

        content = context.content
        # channel = e_context["channel"]
        msg = e_context["context"]["msg"]
        chat_id = msg.from_user_id
        is_group = msg.is_group

        # æ‰“å°å‰50ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
        preview = content[:50] + "..." if len(content) > 50 else content
        logger.info(f"[JinaSum] å¤„ç†æ¶ˆæ¯: {preview}, ç±»å‹={context.type}")

        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºXMLæ ¼å¼ï¼ˆå“”å“©å“”å“©ç­‰ç¬¬ä¸‰æ–¹åˆ†äº«å¡ç‰‡ï¼‰
        if content.startswith("<?xml") or (content.startswith("<msg>") and "<appmsg" in content) or ("<appmsg" in content and "<url>" in content):
            logger.info("[JinaSum] æ£€æµ‹åˆ°XMLæ ¼å¼åˆ†äº«å¡ç‰‡ï¼Œå°è¯•æå–URL")
            try:
                # å¤„ç†å¯èƒ½çš„XMLå£°æ˜
                if content.startswith("<?xml"):
                    content = content[content.find("<msg>") :]

                # å¦‚æœä¸æ˜¯å®Œæ•´çš„XMLï¼Œå°è¯•æ·»åŠ æ ¹èŠ‚ç‚¹
                if not content.startswith("<msg") and "<appmsg" in content:
                    content = f"<msg>{content}</msg>"

                # å¯¹äºä¸€äº›å¯èƒ½æ ¼å¼ä¸æ ‡å‡†çš„XMLï¼Œä½¿ç”¨æ›´å®½æ¾çš„è§£ææ–¹å¼
                try:
                    root = ET.fromstring(content)
                except ET.ParseError:
                    # å°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–URL

                    url_match = re.search(r"<url>(.*?)</url>", content)
                    if url_match:
                        extracted_url = url_match.group(1)
                        logger.info(f"[JinaSum] é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                        content = extracted_url
                        context.type = ContextType.SHARING
                        context.content = extracted_url
                    else:
                        logger.error("[JinaSum] æ— æ³•é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»XMLä¸­æå–URL")
                        return
                else:
                    # XMLè§£ææˆåŠŸ
                    url_elem = root.find(".//url")
                    title_elem = root.find(".//title")

                    # æ£€æŸ¥æ˜¯å¦æœ‰appinfoèŠ‚ç‚¹ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºBç«™ç­‰ç‰¹æ®Šåº”ç”¨
                    appinfo = root.find(".//appinfo")
                    app_name = None
                    if appinfo is not None and appinfo.find("appname") is not None:
                        app_name = appinfo.find("appname").text
                        logger.info(f"[JinaSum] æ£€æµ‹åˆ°APPåˆ†äº«: {app_name}")

                    logger.info(f"[JinaSum] XMLè§£æç»“æœ: url_elem={url_elem is not None}, title_elem={title_elem is not None}, app_name={app_name}")

                    if url_elem is not None and url_elem.text:
                        # æå–åˆ°URLï¼Œå°†ç±»å‹ä¿®æ”¹ä¸ºSHARING
                        extracted_url = url_elem.text
                        logger.info(f"[JinaSum] ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                        content = extracted_url
                        context.type = ContextType.SHARING
                        context.content = extracted_url

                        # å¯¹äºBç«™è§†é¢‘é“¾æ¥ï¼Œè®°å½•é¢å¤–ä¿¡æ¯
                        if app_name and ("å“”å“©å“”å“©" in app_name or "bilibili" in app_name.lower() or "bç«™" in app_name):
                            logger.info("[JinaSum] æ£€æµ‹åˆ°Bç«™è§†é¢‘åˆ†äº«")
                            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ Bç«™è§†é¢‘çš„ç‰¹æ®Šå¤„ç†é€»è¾‘
                    else:
                        logger.error("[JinaSum] æ— æ³•ä»XMLä¸­æå–URL")
                        return
            except Exception as e:
                logger.error(f"[JinaSum] è§£æXMLå¤±è´¥: {str(e)}", exc_info=True)
                return

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨æ€»ç»“
        should_auto_sum = self.auto_sum
        if should_auto_sum and is_group and msg.from_user_nickname in self.black_group_list:
            should_auto_sum = False

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._clean_expired_cache()

        # å¤„ç†åˆ†äº«æ¶ˆæ¯
        if context.type == ContextType.SHARING:
            logger.debug("[JinaSum] Processing SHARING message")
            if is_group:
                if should_auto_sum:
                    return self._process_summary(content, e_context, retry_count=0)
                else:
                    self.pending_messages[chat_id] = {"content": content, "timestamp": time.time()}
                    logger.debug(f"[JinaSum] Cached SHARING message: {content}, chat_id={chat_id}")
                    return
            else:  # å•èŠæ¶ˆæ¯ç›´æ¥å¤„ç†
                return self._process_summary(content, e_context, retry_count=0)

        # å¤„ç†æ–‡æœ¬æ¶ˆæ¯
        elif context.type == ContextType.TEXT:
            logger.debug("[JinaSum] Processing TEXT message")
            content = content.strip()

            # ç§»é™¤å¯èƒ½çš„@ä¿¡æ¯
            if content.startswith("@"):
                parts = content.split(" ", 1)
                if len(parts) > 1:
                    content = parts[1].strip()
                else:
                    content = ""

            # æ£€æŸ¥æ˜¯å¦åŒ…å«"æ€»ç»“"å…³é”®è¯ï¼ˆä»…ç¾¤èŠéœ€è¦ï¼‰
            if is_group and "æ€»ç»“" in content:
                logger.debug(f"[JinaSum] Found summary trigger, pending_messages={self.pending_messages}")
                if chat_id in self.pending_messages:
                    cached_content = self.pending_messages[chat_id]["content"]
                    logger.debug(f"[JinaSum] Processing cached content: {cached_content}")
                    del self.pending_messages[chat_id]
                    return self._process_summary(cached_content, e_context, retry_count=0, skip_notice=False)

                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥URLæ€»ç»“ï¼Œç§»é™¤"æ€»ç»“"å¹¶æ£€æŸ¥å‰©ä½™å†…å®¹æ˜¯å¦ä¸ºURL
                url = content.replace("æ€»ç»“", "").strip()
                if url and self._check_url(url):
                    logger.debug(f"[JinaSum] Processing direct URL: {url}")
                    return self._process_summary(url, e_context, retry_count=0, skip_notice=False)
                logger.debug("[JinaSum] No content to summarize")
                return

            # å¤„ç†"é—®xxx"æ ¼å¼çš„è¿½é—®
            if content.startswith("é—®"):
                question = content[1:].strip()
                if question:
                    logger.debug(f"[JinaSum] Processing question: {question}")
                    return self._process_question(question, chat_id, e_context)
                else:
                    logger.debug("[JinaSum] Empty question, ignored")
                    return

            # å•èŠä¸­ç›´æ¥å¤„ç†URL
            if not is_group:
                url = content.replace("æ€»ç»“", "").strip()
                if url and self._check_url(url):
                    logger.debug(f"[JinaSum] Processing direct URL: {url}")
                    return self._process_summary(url, e_context, retry_count=0)
                logger.debug("[JinaSum] No content to summarize")
                return

    def _clean_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜"""
        current_time = time.time()
        # æ¸…ç†å¾…å¤„ç†æ¶ˆæ¯ç¼“å­˜
        expired_keys = [k for k, v in self.pending_messages.items() if current_time - v["timestamp"] > self.cache_timeout]
        for k in expired_keys:
            del self.pending_messages[k]

    def _extract_wechat_article(self, url, headers):
        """ä¸“é—¨å¤„ç†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« 

        Args:
            url: å¾®ä¿¡æ–‡ç« URL
            headers: è¯·æ±‚å¤´

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æ·»åŠ å¿…è¦çš„å¾®ä¿¡Cookieå‚æ•°ï¼Œå‡å°‘è¢«æ£€æµ‹çš„å¯èƒ½æ€§
            cookies = {
                "appmsglist_action_3941382959": "card",  # ä¸€äº›éšæœºçš„Cookieå€¼
                "appmsglist_action_3941382968": "card",
                "pac_uid": f"{int(time.time())}_f{random.randint(10000, 99999)}",
                "rewardsn": "",
                "wxtokenkey": f"{random.randint(100000, 999999)}",
            }

            # ç›´æ¥ä½¿ç”¨requestsè¿›è¡Œå†…å®¹è·å–ï¼Œæœ‰æ—¶æ¯”newspaperæ›´æœ‰æ•ˆ
            session = requests.Session()
            response = session.get(url, headers=headers, cookies=cookies, timeout=20)
            response.raise_for_status()

            # ä½¿ç”¨BeautifulSoupç›´æ¥è§£æ
            soup = BeautifulSoup(response.content, "html.parser")

            # å¾®ä¿¡æ–‡ç« é€šå¸¸æœ‰è¿™äº›ç‰¹å¾
            title_elem = soup.select_one("#activity-name")
            author_elem = soup.select_one("#js_name") or soup.select_one("#js_profile_qrcode > div > strong")
            content_elem = soup.select_one("#js_content")

            if content_elem:
                # ç§»é™¤æ— ç”¨å…ƒç´ 
                for remove_elem in content_elem.select("script, style, svg"):
                    remove_elem.extract()

                # å°è¯•è·å–æ‰€æœ‰æ–‡æœ¬
                text_content = content_elem.get_text(separator="\n", strip=True)

                if text_content and len(text_content) > 200:  # å†…å®¹è¶³å¤Ÿé•¿
                    title = title_elem.get_text(strip=True) if title_elem else ""
                    author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"

                    # æ„å»ºå®Œæ•´å†…å®¹
                    full_content = ""
                    if title:
                        full_content += f"æ ‡é¢˜: {title}\n"
                    if author and author != "æœªçŸ¥ä½œè€…":
                        full_content += f"ä½œè€…: {author}\n"
                    full_content += f"\n{text_content}"

                    logger.debug(f"[JinaSum] æˆåŠŸé€šè¿‡ç›´æ¥è¯·æ±‚æå–å¾®ä¿¡æ–‡ç« å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                    return full_content
        except Exception as e:
            logger.error(f"[JinaSum] ç›´æ¥è¯·æ±‚æå–å¾®ä¿¡æ–‡ç« å¤±è´¥: {str(e)}")

        return None

    def _extract_bilibili_video(self, url, title):
        """å¤„ç†Bç«™è§†é¢‘å†…å®¹æå–

        Args:
            url: Bç«™è§†é¢‘URL
            title: è§†é¢‘æ ‡é¢˜(å¦‚æœå·²çŸ¥)

        Returns:
            str: æå–çš„å†…å®¹ï¼Œé€šå¸¸åªåŒ…å«æ ‡é¢˜å’Œæç¤ºä¿¡æ¯
        """
        if title:
            return f"æ ‡é¢˜: {title}\n\næè¿°: è¿™æ˜¯ä¸€ä¸ªBç«™è§†é¢‘ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹ã€‚è¯·ç›´æ¥è§‚çœ‹è§†é¢‘ã€‚"
        else:
            return "è¿™æ˜¯ä¸€ä¸ªBç«™è§†é¢‘é“¾æ¥ã€‚ç”±äºè§†é¢‘å†…å®¹æ— æ³•ç›´æ¥æå–ï¼Œè¯·ç›´æ¥ç‚¹å‡»é“¾æ¥è§‚çœ‹è§†é¢‘ã€‚"

    def _extract_with_newspaper(self, url, user_agent):
        """ä½¿ç”¨newspaperåº“æå–æ–‡ç« å†…å®¹

        Args:
            url: æ–‡ç« URL
            user_agent: ä½¿ç”¨çš„User-Agent

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # é…ç½®newspaper
            newspaper.Config().browser_user_agent = user_agent
            newspaper.Config().request_timeout = 30
            newspaper.Config().fetch_images = False  # ä¸ä¸‹è½½å›¾ç‰‡ä»¥åŠ å¿«é€Ÿåº¦
            newspaper.Config().memoize_articles = False  # é¿å…ç¼“å­˜å¯¼è‡´çš„é—®é¢˜

            # åˆ›å»ºArticleå¯¹è±¡ä½†ä¸ç«‹å³ä¸‹è½½
            article = Article(url, language="zh")

            # æ‰‹åŠ¨ä¸‹è½½
            try:
                # æ„å»ºæ›´çœŸå®çš„è¯·æ±‚å¤´
                headers = {
                    "User-Agent": user_agent,
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Sec-Fetch-User": "?1",
                    "Cache-Control": "max-age=0",
                }

                session = requests.Session()
                response = session.get(url, headers=headers, timeout=30)
                response.raise_for_status()

                # æ‰‹åŠ¨è®¾ç½®htmlå†…å®¹
                article.html = response.text
                article.download_state = 2  # è¡¨ç¤ºä¸‹è½½å®Œæˆ
            except Exception as direct_dl_error:
                logger.error(f"[JinaSum] å°è¯•å®šåˆ¶ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {str(direct_dl_error)}")
                article.download()

            # è§£ææ–‡ç« 
            article.parse()

            # å°è¯•è·å–å®Œæ•´å†…å®¹
            title = article.title
            authors = ", ".join(article.authors) if article.authors else "æœªçŸ¥ä½œè€…"
            publish_date = article.publish_date.strftime("%Y-%m-%d") if article.publish_date else "æœªçŸ¥æ—¥æœŸ"
            content = article.text

            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå°è¯•ç›´æ¥ä»HTMLè·å–
            if not content or len(content) < 500:
                logger.debug("[JinaSum] Article content too short, trying to extract from HTML directly")
                content = self._extract_from_html_directly(article.html)

            # åˆæˆæœ€ç»ˆå†…å®¹
            if title:
                full_content = f"æ ‡é¢˜: {title}\n"
                if authors and authors != "æœªçŸ¥ä½œè€…":
                    full_content += f"ä½œè€…: {authors}\n"
                if publish_date and publish_date != "æœªçŸ¥æ—¥æœŸ":
                    full_content += f"å‘å¸ƒæ—¥æœŸ: {publish_date}\n"
                full_content += f"\n{content}"
            else:
                full_content = content

            if not full_content or len(full_content.strip()) < 50:
                logger.debug("[JinaSum] No content extracted by newspaper")
                return None

            logger.debug(f"[JinaSum] Successfully extracted content via newspaper, length: {len(full_content)}")
            return full_content
        except Exception as e:
            logger.error(f"[JinaSum] Error extracting content via newspaper: {str(e)}")
            return None

    def _extract_from_html_directly(self, html_content):
        """ç›´æ¥ä»HTMLå†…å®¹æå–æ–‡æœ¬

        Args:
            html_content: ç½‘é¡µHTMLå†…å®¹

        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            soup = BeautifulSoup(html_content, "html.parser")

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style"]):
                script.extract()

            # è·å–æ‰€æœ‰æ–‡æœ¬
            text = soup.get_text(separator="\n", strip=True)
            return text
        except Exception as bs_error:
            logger.error(f"[JinaSum] BeautifulSoup extraction failed: {str(bs_error)}")
            return ""

    def _resolve_b23_short_url(self, url):
        """è§£æBç«™çŸ­é“¾æ¥è·å–çœŸå®URL

        Args:
            url: Bç«™çŸ­é“¾æ¥

        Returns:
            str: è§£æåçš„çœŸå®URLï¼Œå¤±è´¥è¿”å›åŸå§‹URL
        """
        try:
            logger.debug(f"[JinaSum] Resolving Bç«™çŸ­é“¾æ¥: {url}")
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Cache-Control": "max-age=0",
                "Connection": "keep-alive",
            }
            response = requests.head(url, headers=headers, allow_redirects=True, timeout=10)
            if response.status_code == 200:
                real_url = response.url
                logger.debug(f"[JinaSum] Bç«™çŸ­é“¾æ¥è§£æç»“æœ: {real_url}")
                return real_url
        except Exception as e:
            logger.error(f"[JinaSum] è§£æBç«™çŸ­é“¾æ¥å¤±è´¥: {str(e)}")

        return url

    def _get_content_via_newspaper(self, url):
        """ä½¿ç”¨newspaper3kåº“æå–æ–‡ç« å†…å®¹ï¼Œä¼˜åŒ–ç‰ˆæœ¬

        Args:
            url: æ–‡ç« URL

        Returns:
            str: æ–‡ç« å†…å®¹,å¤±è´¥è¿”å›None
        """
        try:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ
            if url in self.content_cache:
                cached_data = self.content_cache[url]
                if time.time() - cached_data["timestamp"] <= self.cache_timeout:
                    logger.debug(f"[JinaSum] ä½¿ç”¨ç¼“å­˜å†…å®¹ï¼ŒURL: {url}")
                    return cached_data["content"]
                else:
                    del self.content_cache[url]

            # å¤„ç†Bç«™çŸ­é“¾æ¥
            if "b23.tv" in url:
                url = self._resolve_b23_short_url(url)

            # éšæœºé€‰æ‹©ä¸€ä¸ªUser-Agentï¼Œæ¨¡æ‹Ÿä¸åŒæµè§ˆå™¨
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
                "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
            ]
            selected_ua = random.choice(user_agents)

            # æ„å»ºæ›´çœŸå®çš„è¯·æ±‚å¤´
            headers = {
                "User-Agent": selected_ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
            }

            # è®¾ç½®ä¸€ä¸ªéšæœºçš„å¼•èæ¥æºï¼Œå¾®ä¿¡æ–‡ç« æœ‰æ—¶éœ€è¦Referer
            referers = [
                "https://www.baidu.com/",
                "https://www.google.com/",
                "https://www.bing.com/",
                "https://mp.weixin.qq.com/",
                "https://weixin.qq.com/",
                "https://www.qq.com/",
            ]
            if random.random() > 0.3:  # 70%çš„æ¦‚ç‡æ·»åŠ Referer
                headers["Referer"] = random.choice(referers)

            # ä¸ºå¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ·»åŠ ç‰¹æ®Šå¤„ç†
            if "mp.weixin.qq.com" in url:
                wechat_content = self._extract_wechat_article(url, headers)
                if wechat_content:
                    return wechat_content
                # å¦‚æœç‰¹æ®Šå¤„ç†å¤±è´¥ï¼Œä¼šç»§ç»­ä½¿ç”¨newspaperå°è¯•

            # ä½¿ç”¨newspaperæå–å†…å®¹
            extracted_content = self._extract_with_newspaper(url, selected_ua)
            if extracted_content:
                # å¯¹äºBç«™è§†é¢‘ï¼Œç‰¹æ®Šå¤„ç†
                if "bilibili.com" in url or "b23.tv" in url:
                    title = None
                    if extracted_content.startswith("æ ‡é¢˜:"):
                        title_line = extracted_content.split("\n")[0]
                        title = title_line.replace("æ ‡é¢˜:", "").strip()

                    if title and not extracted_content or len(extracted_content.split("\n\n")[-1].strip()) < 100:
                        return self._extract_bilibili_video(url, title)

                return extracted_content

            # å°è¯•ä½¿ç”¨é€šç”¨å†…å®¹æå–æ–¹æ³•ä½œä¸ºå¤‡ç”¨
            content = self._extract_content_general(url, headers)
            if content:
                return content

            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œæä¾›ç‰¹å®šç½‘ç«™çš„é”™è¯¯ä¿¡æ¯
            if "mp.weixin.qq.com" in url:
                return f"æ— æ³•è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹ã€‚å¯èƒ½åŸå› ï¼š\n1. æ–‡ç« éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹\n2. æ–‡ç« å·²è¢«åˆ é™¤\n3. æœåŠ¡å™¨è¢«å¾®ä¿¡é£æ§\n\nè¯·å°è¯•ç›´æ¥æ‰“å¼€é“¾æ¥: {url}"

            # é€šç”¨å¤±è´¥æ¶ˆæ¯
            return None

        except Exception as e:
            logger.error(f"[JinaSum] Error extracting content via newspaper: {str(e)}")

            # å°è¯•é€šç”¨å†…å®¹æå–æ–¹æ³•ä½œä¸ºæœ€åæ‰‹æ®µ
            try:
                content = self._extract_content_general(url)
                if content:
                    return content
            except Exception:
                pass

            return None

    def _try_static_content_extraction(self, url, headers):
        """å°è¯•é™æ€æå–ç½‘é¡µå†…å®¹

        Args:
            url: ç½‘é¡µURL
            headers: è¯·æ±‚å¤´

        Returns:
            str æˆ– None: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # åˆ›å»ºä¼šè¯å¯¹è±¡
            session = requests.Session()

            # è®¾ç½®åŸºæœ¬cookies
            session.cookies.update(
                {
                    f"visit_id_{int(time.time())}": f"{random.randint(1000000, 9999999)}",
                    "has_visited": "1",
                }
            )

            # å‘é€è¯·æ±‚è·å–é¡µé¢
            logger.debug(f"[JinaSum] é€šç”¨æå–æ–¹æ³•æ­£åœ¨è¯·æ±‚: {url}")
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            # ç¡®ä¿ç¼–ç æ­£ç¡®
            if response.encoding == "ISO-8859-1":
                response.encoding = response.apparent_encoding

            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(response.text, "html.parser")

            # ç§»é™¤æ— ç”¨å…ƒç´ 
            for element in soup(["script", "style", "nav", "header", "footer", "aside", "form", "iframe"]):
                element.extract()

            # è·å–æ ‡é¢˜å’Œå†…å®¹
            title = self._find_title(soup)
            content_element = self._find_best_content(soup)

            # å¦‚æœæ‰¾åˆ°å†…å®¹å…ƒç´ ï¼Œæå–å¹¶æ¸…ç†æ–‡æœ¬
            if content_element:
                # ç§»é™¤å†…å®¹ä¸­å¯èƒ½çš„å¹¿å‘Šæˆ–æ— å…³å…ƒç´ 
                for ad in content_element.select('[class*="ad" i], [class*="banner" i], [id*="ad" i], [class*="recommend" i]'):
                    ad.extract()

                # è·å–å¹¶æ¸…ç†æ–‡æœ¬
                content_text = content_element.get_text(separator="\n", strip=True)

                # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
                content_text = re.sub(r"\n{3,}", "\n\n", content_text)

                # æ„å»ºæœ€ç»ˆè¾“å‡º
                result = ""
                if title:
                    result += f"æ ‡é¢˜: {title}\n\n"

                result += content_text

                logger.debug(f"[JinaSum] é€šç”¨æå–æ–¹æ³•æˆåŠŸï¼Œæå–å†…å®¹é•¿åº¦: {len(result)}")
                return result

            return None
        except Exception as e:
            logger.debug(f"[JinaSum] é™æ€æå–å¤±è´¥: {str(e)}")
            return None

    def _find_title(self, soup):
        """ä»BeautifulSoupå¯¹è±¡ä¸­æ‰¾åˆ°æœ€ä½³æ ‡é¢˜

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str æˆ– None: æ‰¾åˆ°çš„æ ‡é¢˜ï¼Œæ²¡æ‰¾åˆ°è¿”å›None
        """
        # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
        title_candidates = [
            soup.select_one("h1"),  # æœ€å¸¸è§çš„æ ‡é¢˜æ ‡ç­¾
            soup.select_one("title"),  # HTMLæ ‡é¢˜
            soup.select_one(".title"),  # å¸¸è§çš„æ ‡é¢˜ç±»
            soup.select_one(".article-title"),  # å¸¸è§çš„æ–‡ç« æ ‡é¢˜ç±»
            soup.select_one(".post-title"),  # åšå®¢æ ‡é¢˜
            soup.select_one('[class*="title" i]'),  # åŒ…å«titleçš„ç±»
        ]

        for candidate in title_candidates:
            if candidate and candidate.text.strip():
                return candidate.text.strip()

        return None

    def _find_best_content(self, soup):
        """æŸ¥æ‰¾ç½‘é¡µä¸­æœ€ä½³å†…å®¹å…ƒç´ 

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            BeautifulSoupå…ƒç´  æˆ– None: æ‰¾åˆ°çš„æœ€ä½³å†…å®¹å…ƒç´ ï¼Œæ²¡æ‰¾åˆ°è¿”å›None
        """
        # æŸ¥æ‰¾å¯èƒ½çš„å†…å®¹å…ƒç´ 
        content_candidates = []

        # 1. å°è¯•æ‰¾å¸¸è§çš„å†…å®¹å®¹å™¨
        content_selectors = [
            "article",
            "main",
            ".content",
            ".article",
            ".post-content",
            '[class*="content" i]',
            '[class*="article" i]',
            ".story",
            ".entry-content",
            ".post-body",
            "#content",
            "#article",
            ".body",
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content_candidates.extend(elements)

        # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å†…å®¹å®¹å™¨ï¼Œå¯»æ‰¾å…·æœ‰æœ€å¤šæ–‡æœ¬çš„divå…ƒç´ 
        if not content_candidates:
            paragraphs = {}
            # æŸ¥æ‰¾æ‰€æœ‰æ®µè½å’Œdiv
            for elem in soup.find_all(["p", "div"]):
                text = elem.get_text(strip=True)
                # åªè€ƒè™‘æœ‰å®é™…å†…å®¹çš„å…ƒç´ 
                if len(text) > 100:
                    paragraphs[elem] = len(text)

            # æ‰¾å‡ºæ–‡æœ¬æœ€å¤šçš„å…ƒç´ 
            if paragraphs:
                max_elem = max(paragraphs.items(), key=lambda x: x[1])[0]
                # å¦‚æœæ˜¯divï¼Œç›´æ¥æ·»åŠ ï¼›å¦‚æœæ˜¯pï¼Œå°è¯•æ‰¾å…¶çˆ¶å…ƒç´ 
                if max_elem.name == "div":
                    content_candidates.append(max_elem)
                else:
                    # æ‰¾åŒ…å«å¤šä¸ªæ®µè½çš„çˆ¶å…ƒç´ 
                    parent = max_elem.parent
                    if parent and len(parent.find_all("p")) > 3:
                        content_candidates.append(parent)
                    else:
                        content_candidates.append(max_elem)

        # 3. ç®€å•ç®—æ³•æ¥è¯„åˆ†å’Œé€‰æ‹©æœ€ä½³å†…å®¹å…ƒç´ 
        return self._score_content_elements(content_candidates)

    def _score_content_elements(self, content_candidates):
        """å¯¹å†…å®¹å€™é€‰å…ƒç´ è¿›è¡Œè¯„åˆ†ï¼Œè¿”å›æœ€ä½³å†…å®¹å…ƒç´ 

        Args:
            content_candidates: å†…å®¹å€™é€‰å…ƒç´ åˆ—è¡¨

        Returns:
            BeautifulSoupå…ƒç´  æˆ– None: æ‰¾åˆ°çš„æœ€ä½³å†…å®¹å…ƒç´ ï¼Œæ²¡æ‰¾åˆ°è¿”å›None
        """
        best_content = None
        max_score = 0

        for element in content_candidates:
            # è®¡ç®—æ–‡æœ¬é•¿åº¦
            text = element.get_text(strip=True)
            text_length = len(text)

            # è®¡ç®—æ–‡æœ¬å¯†åº¦ï¼ˆæ–‡æœ¬é•¿åº¦/HTMLé•¿åº¦ï¼‰
            html_length = len(str(element))
            text_density = text_length / html_length if html_length > 0 else 0

            # è®¡ç®—æ®µè½æ•°é‡
            paragraphs = element.find_all("p")
            paragraph_count = len(paragraphs)

            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
            images = element.find_all("img")
            image_count = len(images)

            # æ ¹æ®å„ç§ç‰¹å¾è®¡ç®—åˆ†æ•°
            score = (
                text_length * 1.0  # æ–‡æœ¬é•¿åº¦å¾ˆé‡è¦
                + text_density * 100  # æ–‡æœ¬å¯†åº¦å¾ˆé‡è¦
                + paragraph_count * 30  # æ®µè½æ•°é‡ä¹Ÿå¾ˆé‡è¦
                + image_count * 10  # å›¾ç‰‡ä¸å¤ªé‡è¦ï¼Œä½†ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‡æ ‡
            )

            # å‡åˆ†é¡¹ï¼šå¦‚æœåŒ…å«è®¸å¤šé“¾æ¥ï¼Œå¯èƒ½æ˜¯å¯¼èˆªæˆ–ä¾§è¾¹æ 
            links = element.find_all("a")
            link_text_ratio = sum(len(a.get_text(strip=True)) for a in links) / text_length if text_length > 0 else 0
            if link_text_ratio > 0.5:  # å¦‚æœé“¾æ¥æ–‡æœ¬å æ¯”è¿‡é«˜
                score *= 0.5

            # æ›´æ–°æœ€ä½³å†…å®¹
            if score > max_score:
                max_score = score
                best_content = element

        return best_content

    def _extract_content_general(self, url, headers=None):
        """é€šç”¨ç½‘é¡µå†…å®¹æå–æ–¹æ³•ï¼Œæ”¯æŒé™æ€å’ŒåŠ¨æ€é¡µé¢ï¼Œä¼˜åŒ–ç‰ˆæœ¬

        é¦–å…ˆå°è¯•é™æ€æå–ï¼ˆæ›´å¿«ã€æ›´è½»é‡ï¼‰ï¼Œå¦‚æœå¤±è´¥æˆ–å†…å®¹å¤ªå°‘å†å°è¯•åŠ¨æ€æå–ï¼ˆæ›´æ…¢ä½†æ›´å¼ºå¤§ï¼‰

        Args:
            url: ç½‘é¡µURL
            headers: å¯é€‰çš„è¯·æ±‚å¤´ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # å¦‚æœæ˜¯ç™¾åº¦æ–‡ç« é“¾æ¥ï¼Œä½¿ç”¨ä¸“é—¨çš„å¤„ç†æ–¹æ³•
            if "md.mbd.baidu.com" in url or "mbd.baidu.com" in url:
                # ç›´æ¥ä½¿ç”¨ä¸“é—¨çš„ç™¾åº¦æ–‡ç« æå–æ–¹æ³•
                content = self._extract_baidu_article(url)
                if content:
                    return content

            # å¦‚æœæ²¡æœ‰æä¾›headersï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
            if not headers:
                headers = self._get_default_headers()

            # æ·»åŠ éšæœºå»¶è¿Ÿä»¥é¿å…è¢«æ£€æµ‹ä¸ºçˆ¬è™«
            time.sleep(random.uniform(0.5, 2))

            # å°è¯•é™æ€æå–å†…å®¹
            static_content_result = self._try_static_content_extraction(url, headers)

            # åˆ¤æ–­é™æ€æå–çš„å†…å®¹è´¨é‡
            content_is_good = False
            if static_content_result:
                # å†…å®¹é•¿åº¦æ£€æŸ¥
                if len(static_content_result) > 1000:
                    content_is_good = True
                # ç»“æ„æ£€æŸ¥ - è‡³å°‘åº”è¯¥æœ‰å¤šä¸ªæ®µè½
                elif static_content_result.count("\n\n") >= 3:
                    content_is_good = True

            # å¦‚æœé™æ€æå–å†…å®¹è´¨é‡ä¸ä½³ï¼Œå°è¯•åŠ¨æ€æå–
            if not content_is_good:
                logger.debug("[JinaSum] é™æ€æå–å†…å®¹è´¨é‡ä¸ä½³ï¼Œå°è¯•åŠ¨æ€æå–")
                dynamic_content = self._extract_dynamic_content(url, headers)
                if dynamic_content:
                    logger.debug(f"[JinaSum] åŠ¨æ€æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(dynamic_content)}")
                    return dynamic_content

            return static_content_result

        except Exception as e:
            logger.error(f"[JinaSum] é€šç”¨å†…å®¹æå–æ–¹æ³•å¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_dynamic_content(self, url, headers=None):
        """ä½¿ç”¨JavaScriptæ¸²æŸ“æå–åŠ¨æ€é¡µé¢å†…å®¹ï¼Œä¼˜åŒ–ç‰ˆæœ¬

        Args:
            url: ç½‘é¡µURL
            headers: å¯é€‰çš„è¯·æ±‚å¤´

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        session = None
        try:
            logger.debug(f"[JinaSum] å¼€å§‹åŠ¨æ€æå–å†…å®¹: {url}")

            # åˆ›å»ºä¼šè¯å¹¶è®¾ç½®è¶…æ—¶
            session = HTMLSession()

            # æ·»åŠ è¯·æ±‚å¤´
            req_headers = headers or self._get_default_headers()

            # è·å–é¡µé¢
            response = session.get(url, headers=req_headers, timeout=30)

            # æ‰§è¡ŒJavaScript (è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢æ— é™ç­‰å¾…)
            logger.debug("[JinaSum] å¼€å§‹æ‰§è¡ŒJavaScript")
            response.html.render(timeout=20, sleep=2)
            logger.debug("[JinaSum] JavaScriptæ‰§è¡Œå®Œæˆ")

            # å¤„ç†æ¸²æŸ“åçš„HTML
            rendered_html = response.html.html

            # è§£ææ¸²æŸ“åçš„HTMLå¹¶æå–å†…å®¹
            content = self._extract_content_from_rendered_html(rendered_html)

            # å…³é—­ä¼šè¯
            if session:
                session.close()

            return content

        except Exception as e:
            logger.error(f"[JinaSum] åŠ¨æ€æå–å¤±è´¥: {str(e)}", exc_info=True)
            # ç¡®ä¿ä¼šè¯è¢«å…³é—­
            if session:
                try:
                    session.close()
                except Exception:
                    pass
            return None

    def _extract_content_from_rendered_html(self, rendered_html):
        """ä»æ¸²æŸ“åçš„HTMLä¸­æå–å†…å®¹

        Args:
            rendered_html: æ¸²æŸ“åçš„HTMLå†…å®¹

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # ä½¿ç”¨BeautifulSoupè§£ææ¸²æŸ“åçš„HTML
            soup = BeautifulSoup(rendered_html, "html.parser")

            # æ¸…ç†æ— ç”¨å…ƒç´ 
            for element in soup(["script", "style", "nav", "header", "footer", "aside"]):
                element.extract()

            # æŸ¥æ‰¾æ ‡é¢˜
            title = self._find_title(soup)

            # å¯»æ‰¾ä¸»è¦å†…å®¹
            main_content = self._find_dynamic_content(soup)

            # ä»ä¸»è¦å†…å®¹ä¸­æå–æ–‡æœ¬
            if main_content:
                # æ¸…ç†å¯èƒ½çš„å¹¿å‘Šæˆ–æ— å…³å…ƒç´ 
                for ad in main_content.select('[class*="ad" i], [class*="banner" i], [id*="ad" i], [class*="recommend" i]'):
                    ad.extract()

                # è·å–æ–‡æœ¬
                content_text = main_content.get_text(separator="\n", strip=True)
                content_text = re.sub(r"\n{3,}", "\n\n", content_text)  # æ¸…ç†å¤šä½™ç©ºè¡Œ

                # æ„å»ºæœ€ç»ˆç»“æœ
                result = ""
                if title:
                    result += f"æ ‡é¢˜: {title}\n\n"
                result += content_text

                return result

            return None
        except Exception as e:
            logger.debug(f"[JinaSum] è§£ææ¸²æŸ“HTMLå¤±è´¥: {str(e)}")
            return None

    def _find_dynamic_content(self, soup):
        """ä¸ºåŠ¨æ€æ¸²æŸ“é¡µé¢æ‰¾åˆ°ä¸»è¦å†…å®¹å…ƒç´ 

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            BeautifulSoupå…ƒç´ : æ‰¾åˆ°çš„å†…å®¹å…ƒç´ ï¼Œå¤±è´¥è¿”å›None
        """
        # 1. å°è¯•æ‰¾ä¸»è¦å†…å®¹å®¹å™¨
        main_selectors = ["article", "main", ".content", ".article", '[class*="content" i]', '[class*="article" i]', "#content", "#article"]

        for selector in main_selectors:
            elements = soup.select(selector)
            if elements:
                # é€‰æ‹©åŒ…å«æœ€å¤šæ–‡æœ¬çš„å…ƒç´ 
                return max(elements, key=lambda x: len(x.get_text()))

        # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå¯»æ‰¾æ–‡æœ¬æœ€å¤šçš„div
        paragraphs = {}
        for elem in soup.find_all(["div"]):
            text = elem.get_text(strip=True)
            if len(text) > 200:  # åªè€ƒè™‘é•¿æ–‡æœ¬
                paragraphs[elem] = len(text)

        if paragraphs:
            return max(paragraphs.items(), key=lambda x: x[1])[0]

        # 3. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ•´ä¸ªbody
        return soup.body

    def _extract_baidu_article(self, url):
        """ä¸“é—¨ç”¨äºæå–ç™¾åº¦æ–‡ç« å†…å®¹çš„æ–¹æ³•ï¼Œä¼˜åŒ–ç‰ˆæœ¬

        Args:
            url: ç™¾åº¦æ–‡ç« URL

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"[JinaSum] å°è¯•ä¸“é—¨æå–ç™¾åº¦æ–‡ç« : {url}")

            # æå–æ–‡ç« ID
            article_id = self._extract_baidu_article_id(url)
            if not article_id:
                logger.error(f"[JinaSum] æ— æ³•ä»URLæå–ç™¾åº¦æ–‡ç« ID: {url}")
                return None

            logger.debug(f"[JinaSum] æå–åˆ°ç™¾åº¦æ–‡ç« ID: {article_id}")

            # åˆå§‹åŒ–ç§»åŠ¨è®¾å¤‡UAåˆ—è¡¨ä¾›åç»­ä½¿ç”¨
            self.mobile_user_agents = [
                "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1",
                "Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.91 Mobile Safari/537.36",
                "Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/94.0.4606.76 Mobile/15E148 Safari/604.1",
            ]

            # æ„å»ºå¤šç§URLå°è¯•æå–
            url_formats = [
                # å°è¯•ç›´æ¥è®¿é—®åŸå§‹URL
                url,
                # å°è¯•ç§»åŠ¨ç½‘é¡µç‰ˆæ ¼å¼1
                f"https://mbd.baidu.com/newspage/data/landingshare?context=%7B%22nid%22%3A%22news_{article_id}%22%2C%22sourceFrom%22%3A%22bjh%22%7D",
                # å°è¯•ç§»åŠ¨ç½‘é¡µç‰ˆæ ¼å¼2
                f"https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_{article_id}%22%7D",
            ]

            # ä¾æ¬¡å°è¯•æ¯ä¸ªURLæ ¼å¼
            for target_url in url_formats:
                content = self._try_extract_baidu_url(target_url)
                if content:
                    return content

            # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›None
            logger.error("[JinaSum] æ‰€æœ‰ç™¾åº¦æ–‡ç« æå–æ–¹æ³•å‡å¤±è´¥")
            return None

        except Exception as e:
            logger.error(f"[JinaSum] ä¸“é—¨æå–ç™¾åº¦æ–‡ç« å¤±è´¥: {str(e)}")
            return None

    def _extract_baidu_article_id(self, url):
        """ä»ç™¾åº¦æ–‡ç« URLä¸­æå–æ–‡ç« ID

        Args:
            url: ç™¾åº¦æ–‡ç« URL

        Returns:
            str: æ–‡ç« IDï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æå–æ–‡ç« ID
            article_id = None
            parsed_url = urlparse(url)
            path_parts = parsed_url.path.split("/")

            # ä¾‹å¦‚ /r/1A1GKWoodMI
            if len(path_parts) > 1 and path_parts[-2] == "r":
                article_id = path_parts[-1]

            # ä¾‹å¦‚ ?r=1A1GKWoodMI
            if not article_id:
                query_params = parse_qs(parsed_url.query)
                if "r" in query_params:
                    article_id = query_params["r"][0]

            return article_id
        except Exception as e:
            logger.debug(f"[JinaSum] æå–ç™¾åº¦æ–‡ç« IDå¤±è´¥: {str(e)}")
            return None

    def _try_extract_baidu_url(self, target_url):
        """å°è¯•ä»å•ä¸ªç™¾åº¦æ–‡ç« URLä¸­æå–å†…å®¹

        Args:
            target_url: ç™¾åº¦æ–‡ç« URLæ ¼å¼

        Returns:
            str: æˆåŠŸæå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"[JinaSum] å°è¯•ç™¾åº¦æ–‡ç« URLæ ¼å¼: {target_url}")

            # æ„å»ºè¯·æ±‚å¤´
            headers = {
                "User-Agent": random.choice(self.mobile_user_agents),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Connection": "keep-alive",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }

            # å‘é€è¯·æ±‚
            response = requests.get(target_url, headers=headers, timeout=15, allow_redirects=True)
            response.raise_for_status()

            # ç¡®ä¿ç¼–ç æ­£ç¡®
            if response.encoding == "ISO-8859-1":
                response.encoding = response.apparent_encoding

            # æ£€æŸ¥æ˜¯å¦æ˜¯JSONå“åº” - æŸäº›ç™¾åº¦APIä¼šè¿”å›JSON
            content_type = response.headers.get("Content-Type", "")
            if "application/json" in content_type or response.text.strip().startswith("{"):
                return self._extract_from_json(response.text)

            # è§£æHTML
            soup = BeautifulSoup(response.text, "html.parser")

            # å°è¯•æå–JSONæ•°æ®
            json_content = self._extract_from_script_json(soup)
            if json_content:
                return json_content

            # å°è¯•ä»HTMLç›´æ¥æå–å†…å®¹
            html_content = self._extract_from_baidu_html(soup)
            if html_content:
                return html_content

            return None
        except Exception as e:
            logger.debug(f"[JinaSum] å°è¯•URL {target_url} å¤±è´¥: {str(e)}")
            return None

    def _extract_from_json(self, json_text):
        """ä»JSONå“åº”ä¸­æå–ç™¾åº¦æ–‡ç« å†…å®¹

        Args:
            json_text: JSONå“åº”æ–‡æœ¬

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            data = json.loads(json_text)
            # æ£€æŸ¥JSONæ•°æ®ä¸­æ˜¯å¦åŒ…å«æ–‡ç« å†…å®¹
            if data.get("data", {}).get("title") and (data.get("data", {}).get("content") or data.get("data", {}).get("html")):
                title = data["data"]["title"]
                content_html = data["data"].get("content", "") or data["data"].get("html", "")
                author = data["data"].get("author", "")
                publish_time = data["data"].get("publish_time", "")

                # è§£æHTMLå†…å®¹
                content_soup = BeautifulSoup(content_html, "html.parser")

                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for tag in content_soup(["script", "style"]):
                    tag.decompose()

                # æå–çº¯æ–‡æœ¬
                content_text = content_soup.get_text(separator="\n", strip=True)

                # æ„å»ºç»“æœ
                result = f"æ ‡é¢˜: {title}\n"
                if author:
                    result += f"ä½œè€…: {author}\n"
                if publish_time:
                    result += f"æ—¶é—´: {publish_time}\n"

                result += f"\n{content_text}"

                logger.debug(f"[JinaSum] æˆåŠŸé€šè¿‡JSONæå–ç™¾åº¦æ–‡ç« ï¼Œé•¿åº¦: {len(result)}")
                return result
        except json.JSONDecodeError:
            return None
        except Exception as e:
            logger.debug(f"[JinaSum] ä»JSONæå–å†…å®¹å¤±è´¥: {str(e)}")
            return None

    def _extract_from_script_json(self, soup):
        """ä»HTMLä¸­çš„è„šæœ¬æ ‡ç­¾æå–åµŒå…¥JSONæ•°æ®

        Args:
            soup: BeautifulSoupè§£æçš„HTML

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        for script in soup.find_all("script"):
            script_text = script.string
            if not script_text or not ("content" in script_text or "article" in script_text):
                continue

            try:
                # å°è¯•æ‰¾åˆ°JSONæ ¼å¼çš„æ•°æ®
                json_start = script_text.find("{")
                json_end = script_text.rfind("}") + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = script_text[json_start:json_end]
                    data = json.loads(json_str)

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ç« æ•°æ®
                    article_data = None
                    if "article" in data:
                        article_data = data["article"]
                    elif "data" in data and "article" in data["data"]:
                        article_data = data["data"]["article"]

                    if article_data and "title" in article_data:
                        title = article_data.get("title", "")
                        content = article_data.get("content", "")
                        author = article_data.get("author", "")
                        publish_time = article_data.get("publish_time", "")

                        # è§£æHTMLå†…å®¹
                        if content:
                            content_soup = BeautifulSoup(content, "html.parser")
                            content_text = content_soup.get_text(separator="\n", strip=True)

                            # æ„å»ºç»“æœ
                            result = f"æ ‡é¢˜: {title}\n"
                            if author:
                                result += f"ä½œè€…: {author}\n"
                            if publish_time:
                                result += f"æ—¶é—´: {publish_time}\n"

                            result += f"\n{content_text}"

                            logger.debug(f"[JinaSum] æˆåŠŸä»åµŒå…¥JSONæå–ç™¾åº¦æ–‡ç« ï¼Œé•¿åº¦: {len(result)}")
                            return result
            except Exception as e:
                logger.debug(f"[JinaSum] ä»è„šæœ¬æå–JSONå¤±è´¥: {str(e)}")

        return None

    def _extract_from_baidu_html(self, soup):
        """ä»HTMLç›´æ¥æå–ç™¾åº¦æ–‡ç« å†…å®¹

        Args:
            soup: BeautifulSoupè§£æçš„HTML

        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        # æå–æ ‡é¢˜
        title = None
        for selector in [".article-title", ".title", "h1.title", "h1"]:
            title_elem = soup.select_one(selector)
            if title_elem and title_elem.text.strip():
                title = title_elem.text.strip()
                break

        # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä½¿ç”¨æ ‡é¢˜æ ‡ç­¾
        if not title:
            title_tag = soup.find("title")
            if title_tag:
                title = title_tag.text.strip()

        # æå–ä½œè€…
        author = None
        for selector in [".author", ".writer", ".source", ".article-author"]:
            author_elem = soup.select_one(selector)
            if author_elem and author_elem.text.strip():
                author = author_elem.text.strip()
                break

        # æå–å†…å®¹
        content = None
        for selector in [".article-content", ".article-detail", ".content", ".artcle", "#article"]:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤æ— ç”¨å…ƒç´ 
                for remove_elem in content_elem.select(".ad-banner, .recommend, .share-btn, script, style"):
                    remove_elem.extract()

                content_text = content_elem.get_text(separator="\n", strip=True)
                if len(content_text) > 200:  # å†…å®¹è¶³å¤Ÿé•¿
                    content = content_text
                    break

        # å¦‚æœæ²¡æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æŸ¥æ‰¾æœ€é•¿çš„æ®µè½é›†åˆ
        if not content:
            max_paragraphs = []
            max_text_len = 0

            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å†…å®¹å®¹å™¨
            for div in soup.find_all("div"):
                paragraphs = div.find_all("p")
                if len(paragraphs) >= 3:  # è‡³å°‘æœ‰3ä¸ªæ®µè½
                    text = "\n".join([p.get_text(strip=True) for p in paragraphs])
                    if len(text) > max_text_len:
                        max_text_len = len(text)
                        max_paragraphs = paragraphs

            # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿé•¿çš„æ®µè½é›†åˆ
            if max_text_len > 200:
                content = "\n".join([p.get_text(strip=True) for p in max_paragraphs])

        # å¦‚æœæ‰¾åˆ°å†…å®¹ï¼Œæ„å»ºç»“æœ
        if content:
            result = ""
            if title:
                result += f"æ ‡é¢˜: {title}\n"
            if author:
                result += f"ä½œè€…: {author}\n"
            result += f"\n{content}"

            logger.debug(f"[JinaSum] æˆåŠŸé€šè¿‡HTMLæå–ç™¾åº¦æ–‡ç« ï¼Œé•¿åº¦: {len(result)}")
            return result

        return None

    def _get_default_headers(self):
        """è·å–é»˜è®¤è¯·æ±‚å¤´"""

        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
        ]
        selected_ua = random.choice(user_agents)

        return {
            "User-Agent": selected_ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "max-age=0",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
        }

    def _process_summary(self, content: str, e_context: EventContext, retry_count: int = 0, skip_notice: bool = False):
        """å¤„ç†æ€»ç»“è¯·æ±‚ï¼Œä¼˜åŒ–ç‰ˆæœ¬"""
        # æå‰éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ
        if not self._check_url(content):
            logger.debug(f"[JinaSum] {content} is not a valid url, skip")
            return

        # æ˜¾ç¤ºæ­£åœ¨å¤„ç†çš„æç¤º
        if retry_count == 0 and not skip_notice:
            logger.debug("[JinaSum] Processing URL: %s" % content)
            reply = Reply(ReplyType.TEXT, "ğŸ‰æ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆæ€»ç»“ï¼Œè¯·ç¨å€™...")
            channel = e_context["channel"]
            channel.send(reply, e_context["context"])

        try:
            # è·å–ç½‘é¡µå†…å®¹
            target_url = html.unescape(content)
            target_url_content = self._get_web_content(target_url)

            # æ£€æŸ¥è¿”å›çš„å†…å®¹æ˜¯å¦åŒ…å«éªŒè¯æç¤º
            if target_url_content and target_url_content.startswith("âš ï¸"):
                # è¿™æ˜¯ä¸€ä¸ªéªŒè¯æç¤ºï¼Œç›´æ¥è¿”å›ç»™ç”¨æˆ·
                logger.info(f"[JinaSum] è¿”å›éªŒè¯æç¤ºç»™ç”¨æˆ·: {target_url_content}")
                reply = Reply(ReplyType.INFO, target_url_content)
                e_context["reply"] = reply
                e_context.action = EventAction.BREAK_PASS
                return

            # å¦‚æœæå–å†…å®¹å¤±è´¥ï¼Œå°è¯•ç‰¹æ®Šå¤„ç†
            if not target_url_content:
                # å¯¹äºBç«™è§†é¢‘ï¼Œæä¾›ç‰¹æ®Šå¤„ç†
                if "bilibili.com" in target_url or "b23.tv" in target_url:
                    target_url_content = "è¿™æ˜¯ä¸€ä¸ªBç«™è§†é¢‘é“¾æ¥ã€‚ç”±äºè§†é¢‘å†…å®¹æ— æ³•ç›´æ¥æå–ï¼Œè¯·ç›´æ¥ç‚¹å‡»é“¾æ¥è§‚çœ‹è§†é¢‘ã€‚"
                else:
                    raise ValueError("æ— æ³•æå–æ–‡ç« å†…å®¹")

            # æ¸…æ´—å†…å®¹
            target_url_content = self._clean_content(target_url_content)
            # å°†æ¸…æ´—åçš„å†…å®¹å­˜å…¥ç¼“å­˜
            self.content_cache[target_url] = {"content": target_url_content, "timestamp": time.time()}

            # é™åˆ¶å†…å®¹é•¿åº¦
            target_url_content = target_url_content[: self.max_words]
            logger.debug(f"[JinaSum] Got content length: {len(target_url_content)}")

            # æ„é€ æç¤ºè¯å’Œå†…å®¹
            sum_prompt = f"{self.prompt}\n\n'''{target_url_content}'''"

            # è°ƒç”¨OpenAI APIç”Ÿæˆæ€»ç»“
            self._call_openai_api(sum_prompt, e_context)

        except Exception as e:
            logger.error(f"[JinaSum] Error in processing summary: {str(e)}")
            self._handle_summary_error(content, e_context, retry_count, e)

    def _get_web_content(self, url):
        """ä»URLè·å–ç½‘é¡µå†…å®¹ï¼Œå¤„ç†å¯èƒ½çš„XMLæ ¼å¼

        Args:
            url: ç½‘é¡µURLæˆ–XMLå†…å®¹

        Returns:
            str: æå–çš„ç½‘é¡µå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºXMLæ ¼å¼ï¼ˆå“”å“©å“”å“©ç­‰ç¬¬ä¸‰æ–¹åˆ†äº«å¡ç‰‡ï¼‰
        if url.startswith("<?xml") or (url.startswith("<msg>") and "<appmsg" in url) or ("<appmsg" in url and "<url>" in url):
            logger.info("[JinaSum] æ£€æµ‹åˆ°XMLæ ¼å¼åˆ†äº«å¡ç‰‡ï¼Œå°è¯•æå–URL")
            try:
                extracted_url = self._extract_url_from_xml(url)
                if extracted_url:
                    url = extracted_url
                else:
                    logger.error("[JinaSum] æ— æ³•ä»XMLä¸­æå–URL")
                    return None
            except Exception as e:
                logger.error(f"[JinaSum] è§£æXMLå¤±è´¥: {str(e)}", exc_info=True)
                return None

        # ä½¿ç”¨newspaper3kæå–å†…å®¹
        logger.debug(f"[JinaSum] ä½¿ç”¨newspaper3kæå–å†…å®¹: {url}")
        return self._get_content_via_newspaper(url)

    def _extract_url_from_xml(self, xml_content):
        """ä»XMLå†…å®¹ä¸­æå–URL

        Args:
            xml_content: XMLæ ¼å¼çš„å†…å®¹

        Returns:
            str: æå–çš„URLï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # å¤„ç†å¯èƒ½çš„XMLå£°æ˜
            if xml_content.startswith("<?xml"):
                xml_content = xml_content[xml_content.find("<msg>") :]

            # å¦‚æœä¸æ˜¯å®Œæ•´çš„XMLï¼Œå°è¯•æ·»åŠ æ ¹èŠ‚ç‚¹
            if not xml_content.startswith("<msg") and "<appmsg" in xml_content:
                xml_content = f"<msg>{xml_content}</msg>"

            # å°è¯•è§£æXML
            try:
                root = ET.fromstring(xml_content)
                url_elem = root.find(".//url")
                if url_elem is not None and url_elem.text:
                    extracted_url = url_elem.text
                    logger.info(f"[JinaSum] ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                    return extracted_url
            except ET.ParseError:
                # XMLè§£æå¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
                url_match = re.search(r"<url>(.*?)</url>", xml_content)
                if url_match:
                    extracted_url = url_match.group(1)
                    logger.info(f"[JinaSum] é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                    return extracted_url

            return None
        except Exception as e:
            logger.error(f"[JinaSum] æå–URLå¤±è´¥: {str(e)}")
            return None

    def _call_openai_api(self, prompt, e_context):
        """è°ƒç”¨OpenAI APIç”Ÿæˆå†…å®¹æ€»ç»“

        Args:
            prompt: å‘é€ç»™AIæ¨¡å‹çš„æç¤ºè¯
            e_context: äº‹ä»¶ä¸Šä¸‹æ–‡å¯¹è±¡
        """
        try:
            # æ„é€ å®Œæ•´è¯·æ±‚å‚æ•°
            openai_payload = {"model": self.openai_model, "messages": [{"role": "user", "content": prompt}], "temperature": 0.7, "max_tokens": min(2000, self.max_words)}

            # å‘é€APIè¯·æ±‚
            response = requests.post(self._get_openai_chat_url(), headers={"Authorization": f"Bearer {self.openai_api_key}"}, json=openai_payload, timeout=30)
            response.raise_for_status()
            answer = response.json()["choices"][0]["message"]["content"]

            # ä¿®æ”¹contextå†…å®¹
            e_context["context"].type = ContextType.TEXT
            answer = remove_markdown_symbol(answer)
            reply = Reply(ReplyType.TEXT, answer)

            # è®¾ç½®å›å¤å¹¶ä¸­æ–­å¤„ç†é“¾
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS
            logger.debug(f"[JinaSum] ä½¿ç”¨Bridgeç›´æ¥è°ƒç”¨åå°æ¨¡å‹æˆåŠŸï¼Œå›å¤ç±»å‹={reply.type}ï¼Œé•¿åº¦={len(reply.content) if reply.content else 0}")
            return True
        except Exception as e:
            logger.warning(f"[JinaSum] ç›´æ¥è°ƒç”¨åå°å¤±è´¥: {str(e)}", exc_info=True)
            # å¦‚æœç›´æ¥è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°æ’ä»¶é“¾çš„æ–¹å¼
            logger.debug("[JinaSum] å›é€€åˆ°ä½¿ç”¨æ’ä»¶é“¾å¤„ç†")
            e_context.action = EventAction.CONTINUE
            return False

    def _handle_summary_error(self, content, e_context, retry_count, exception=None):
        """å¤„ç†æ€»ç»“è¿‡ç¨‹ä¸­çš„é”™è¯¯

        Args:
            content: åŸå§‹URLå†…å®¹
            e_context: äº‹ä»¶ä¸Šä¸‹æ–‡
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            exception: æ•è·çš„å¼‚å¸¸
        """
        if retry_count < 3:
            logger.info(f"[JinaSum] Retrying {retry_count + 1}/3...")
            return self._process_summary(content, e_context, retry_count + 1, True)

        # å‹å¥½çš„é”™è¯¯æç¤º
        error_msg = "æŠ±æ­‰ï¼Œæ— æ³•è·å–æ–‡ç« å†…å®¹ã€‚å¯èƒ½æ˜¯å› ä¸º:\n"
        error_msg += "1. æ–‡ç« éœ€è¦ç™»å½•æˆ–å·²è¿‡æœŸ\n"
        error_msg += "2. æ–‡ç« æœ‰ç‰¹æ®Šçš„è®¿é—®é™åˆ¶\n"
        error_msg += "3. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n\n"
        error_msg += "å»ºè®®æ‚¨:\n"
        error_msg += "- ç›´æ¥æ‰“å¼€é“¾æ¥æŸ¥çœ‹\n"
        error_msg += "- ç¨åé‡è¯•\n"
        error_msg += "- å°è¯•å…¶ä»–æ–‡ç« "

        reply = Reply(ReplyType.ERROR, error_msg)
        e_context["reply"] = reply
        e_context.action = EventAction.BREAK_PASS

    def _process_question(self, question: str, chat_id: str, e_context: EventContext, retry_count: int = 0):
        """å¤„ç†ç”¨æˆ·æé—®"""
        try:
            # å‚æ•°æ ¡éªŒ
            if not self.openai_api_key:
                raise ValueError("OpenAI APIå¯†é’¥æœªé…ç½®")
            if not question.strip():
                raise ValueError("é—®é¢˜å†…å®¹ä¸ºç©º")

            # è·å–æœ€è¿‘æ€»ç»“çš„å†…å®¹
            recent_content = None
            recent_timestamp = 0

            # éå†æ‰€æœ‰ç¼“å­˜æ‰¾åˆ°æœ€è¿‘æ€»ç»“çš„å†…å®¹
            for url, cache_data in self.content_cache.items():
                if cache_data["timestamp"] > recent_timestamp:
                    recent_timestamp = cache_data["timestamp"]
                    recent_content = cache_data["content"]

            if not recent_content or time.time() - recent_timestamp > self.cache_timeout:
                logger.debug("[JinaSum] No valid content cache found or content expired")
                return  # æ‰¾ä¸åˆ°ç›¸å…³æ–‡ç« ï¼Œè®©åç»­æ’ä»¶å¤„ç†é—®é¢˜

            if retry_count == 0:
                reply = Reply(ReplyType.TEXT, "ğŸ¤” æ­£åœ¨æ€è€ƒæ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™...")
                channel = e_context["channel"]
                channel.send(reply, e_context["context"])

            # æ„å»ºé—®ç­”çš„ prompt
            qa_prompt = self.qa_prompt.format(
                content=recent_content[: self.max_words],  # ä½¿ç”¨å®ä¾‹å˜é‡
                question=question.strip(),
            )

            # æ„é€ å®Œæ•´è¯·æ±‚å‚æ•°
            openai_payload = {"model": self.openai_model, "messages": [{"role": "user", "content": qa_prompt}], "temperature": 0.7, "max_tokens": min(2000, self.max_words)}

            # å‘é€APIè¯·æ±‚
            response = requests.post(self._get_openai_chat_url(), headers={"Authorization": f"Bearer {self.openai_api_key}"}, json=openai_payload, timeout=30)
            response.raise_for_status()
            answer = response.json()["choices"][0]["message"]["content"]

            answer = remove_markdown_symbol(answer)
            reply = Reply(ReplyType.TEXT, answer)
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS

        except Exception as e:
            logger.error(f"[JinaSum] Error in processing question: {str(e)}")
            if retry_count < 3:
                return self._process_question(question, chat_id, e_context, retry_count + 1)
            reply = Reply(ReplyType.ERROR, f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºé”™: {str(e)}")
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS

    def get_help_text(self, verbose, **kwargs):
        help_text = "ç½‘é¡µå†…å®¹æ€»ç»“æ’ä»¶:\n"
        help_text += "1. å‘é€ã€Œæ€»ç»“ ç½‘å€ã€å¯ä»¥æ€»ç»“æŒ‡å®šç½‘é¡µçš„å†…å®¹\n"
        help_text += "2. å•èŠæ—¶åˆ†äº«æ¶ˆæ¯ä¼šè‡ªåŠ¨æ€»ç»“\n"
        if self.auto_sum:
            help_text += "3. ç¾¤èŠä¸­åˆ†äº«æ¶ˆæ¯é»˜è®¤è‡ªåŠ¨æ€»ç»“"
            if self.black_group_list:
                help_text += "ï¼ˆéƒ¨åˆ†ç¾¤ç»„éœ€è¦å‘é€å«ã€Œæ€»ç»“ã€çš„æ¶ˆæ¯è§¦å‘ï¼‰\n"
            else:
                help_text += "\n"
        else:
            help_text += "3. ç¾¤èŠä¸­æ”¶åˆ°åˆ†äº«æ¶ˆæ¯åï¼Œå‘é€åŒ…å«ã€Œæ€»ç»“ã€çš„æ¶ˆæ¯å³å¯è§¦å‘æ€»ç»“\n"
        help_text += f"4. æ€»ç»“å®Œæˆå5åˆ†é’Ÿå†…ï¼Œå¯ä»¥å‘é€ã€Œ{self.qa_trigger}xxxã€æ¥è¯¢é—®æ–‡ç« ç›¸å…³é—®é¢˜\n"
        help_text += "æ³¨ï¼šç¾¤èŠä¸­çš„åˆ†äº«æ¶ˆæ¯çš„æ€»ç»“è¯·æ±‚éœ€è¦åœ¨60ç§’å†…å‘å‡º"
        return help_text

    def _get_openai_chat_url(self):
        return self.openai_api_base + "/chat/completions"

    def _get_openai_headers(self):
        """è·å–openaiçš„header"""
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.openai_api_key}",  # ç›´æ¥ä½¿ç”¨å®ä¾‹å˜é‡
        }

    def _check_url(self, target_url: str):
        """å¢å¼ºURLæ£€æŸ¥"""
        stripped_url = target_url.strip()
        logger.debug(f"[JinaSum] æ£€æŸ¥URL: {stripped_url}")

        # åè®®å¤´æ£€æŸ¥
        if not stripped_url.lower().startswith(("http://", "https://")):
            logger.debug("[JinaSum] URLåè®®å¤´ä¸åˆæ³•")
            return False

        # é»‘åå•æ£€æŸ¥ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        lower_url = stripped_url.lower()
        if any(black.lower() in lower_url for black in self.black_url_list):
            logger.debug("[JinaSum] URLåœ¨é»‘åå•ä¸­")
            return False

        return True

    def _clean_content(self, content: str) -> str:
        """æ¸…æ´—å†…å®¹ï¼Œå»é™¤å›¾ç‰‡ã€é“¾æ¥ã€å¹¿å‘Šç­‰æ— ç”¨ä¿¡æ¯

        Args:
            content: åŸå§‹å†…å®¹

        Returns:
            str: æ¸…æ´—åçš„å†…å®¹
        """
        # è®°å½•åŸå§‹é•¿åº¦
        original_length = len(content)
        logger.debug(f"[JinaSum] Original content length: {original_length}")

        # ç§»é™¤Markdownå›¾ç‰‡æ ‡ç­¾
        content = re.sub(r"!\[.*?\]\(.*?\)", "", content)
        content = re.sub(r"\[!\[.*?\]\(.*?\)", "", content)  # åµŒå¥—å›¾ç‰‡æ ‡ç­¾

        # ç§»é™¤å›¾ç‰‡æè¿° (é€šå¸¸åœ¨æ–¹æ‹¬å·æˆ–ç‰¹å®šæ ¼å¼ä¸­)
        content = re.sub(r"\[å›¾ç‰‡\]|\[image\]|\[img\]|\[picture\]", "", content, flags=re.IGNORECASE)
        content = re.sub(r"\[.*?å›¾ç‰‡.*?\]", "", content)

        # ç§»é™¤é˜…è¯»æ—¶é—´ã€å­—æ•°ç­‰å…ƒæ•°æ®
        content = re.sub(r"æœ¬æ–‡å­—æ•°ï¼š\d+ï¼Œé˜…è¯»æ—¶é•¿å¤§çº¦\d+åˆ†é’Ÿ", "", content)
        content = re.sub(r"é˜…è¯»æ—¶é•¿[:ï¼š].*?åˆ†é’Ÿ", "", content)
        content = re.sub(r"å­—æ•°[:ï¼š]\d+", "", content)

        # ç§»é™¤æ—¥æœŸæ ‡è®°å’Œæ—¶é—´æˆ³
        content = re.sub(r"\d{4}[\.å¹´/-]\d{1,2}[\.æœˆ/-]\d{1,2}[æ—¥å·]?(\s+\d{1,2}:\d{1,2}(:\d{1,2})?)?", "", content)

        # ç§»é™¤åˆ†éš”çº¿
        content = re.sub(r"\*\s*\*\s*\*", "", content)
        content = re.sub(r"-{3,}", "", content)
        content = re.sub(r"_{3,}", "", content)

        # ç§»é™¤ç½‘é¡µä¸­å¸¸è§çš„å¹¿å‘Šæ ‡è®°
        ad_patterns = [
            r"å¹¿å‘Š\s*[\.ã€‚]?",
            r"èµåŠ©å†…å®¹",
            r"sponsored content",
            r"advertisement",
            r"promoted content",
            r"æ¨å¹¿ä¿¡æ¯",
            r"\[å¹¿å‘Š\]",
            r"ã€å¹¿å‘Šã€‘",
        ]
        for pattern in ad_patterns:
            content = re.sub(pattern, "", content, flags=re.IGNORECASE)

        # ç§»é™¤URLé“¾æ¥å’Œç©ºçš„Markdowné“¾æ¥
        content = re.sub(r"https?://\S+", "", content)
        content = re.sub(r"www\.\S+", "", content)
        content = re.sub(r"\[\]\(.*?\)", "", content)  # ç©ºé“¾æ¥å¼•ç”¨ [](...)
        content = re.sub(r"\[.+?\]\(\s*\)", "", content)  # æœ‰æ–‡æœ¬æ— é“¾æ¥ [text]()

        # æ¸…ç†Markdownæ ¼å¼ä½†ä¿ç•™æ–‡æœ¬å†…å®¹
        content = re.sub(r"\*\*(.+?)\*\*", r"\1", content)  # ç§»é™¤åŠ ç²—æ ‡è®°ä½†ä¿ç•™å†…å®¹
        content = re.sub(r"\*(.+?)\*", r"\1", content)  # ç§»é™¤æ–œä½“æ ‡è®°ä½†ä¿ç•™å†…å®¹
        content = re.sub(r"`(.+?)`", r"\1", content)  # ç§»é™¤ä»£ç æ ‡è®°ä½†ä¿ç•™å†…å®¹

        # æ¸…ç†æ–‡ç« å°¾éƒ¨çš„"å¾®ä¿¡ç¼–è¾‘"å’Œ"æ¨èé˜…è¯»"ç­‰æ— å…³å†…å®¹
        content = re.sub(r"\*\*å¾®ä¿¡ç¼–è¾‘\*\*.*?$", "", content, flags=re.MULTILINE)
        content = re.sub(r"\*\*æ¨èé˜…è¯»\*\*.*?$", "", content, flags=re.MULTILINE | re.DOTALL)

        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r"\n{3,}", "\n\n", content)  # ç§»é™¤å¤šä½™ç©ºè¡Œ
        content = re.sub(r"\s{2,}", " ", content)  # ç§»é™¤å¤šä½™ç©ºæ ¼
        content = re.sub(r"^\s+", "", content, flags=re.MULTILINE)  # ç§»é™¤è¡Œé¦–ç©ºç™½
        content = re.sub(r"\s+$", "", content, flags=re.MULTILINE)  # ç§»é™¤è¡Œå°¾ç©ºç™½

        # è®°å½•æ¸…æ´—åé•¿åº¦
        cleaned_length = len(content)
        logger.debug(f"[JinaSum] Cleaned content length: {cleaned_length}, removed {original_length - cleaned_length} characters")

        return content
